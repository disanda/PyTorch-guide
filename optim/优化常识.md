## optim的一些理论基础

神经网络的训练过程如下：

- 做一个神经网络用于拟合目标函数
- 做一个真实值和目标函数值直接估计误差的损失函数，
- 用损失函数值前向输入值求导，
- 再根据导数的反方向去更新网络参数,目的是让损失函数值最终为0.

在第四步中，参数的更新就是pytorch中的optim(优化过程)，现在都是随机梯度下降，即每次更新是通过随机抽样来更新参数(总数的一小部分，简称为一个batch)。

### 1.learning rate

选择适当的学习率才能优化到合适的目标.

学习率过高: 1.目标发散

学习旅过低: 2.更新较慢





### Reference
[1]. AdaDelta:AN ADAPTIVE Learning Rate Method, Matthew D. Zeiler, 2012
