## optim的一些理论基础

神经网络的训练过程如下：

- 做一个神经网络用于拟合目标函数
- 做一个真实值和目标函数值直接估计误差的损失函数，
- 用损失函数值前向输入值求导，
- 再根据导数的反方向去更新网络参数(x),目的是让损失函数值最终为0.

![1](http://latex.codecogs.com/gif.latex?x_{t+1}=x_t+\delta{x_t})

- SGD
>在第四步中，参数的更新就是pytorch中的optim(优化过程)，现在都是随机梯度下降，即每次更新是通过随机抽样来更新参数(总数的一小部分，简称为一个batch)。

- learning rate
选择适当的学习率才能优化到合适的目标.

学习率过高: 1.目标发散

学习旅过低: 2.更新较慢

### 1.Hessian matrix

### 2.Momentum
和学习率相关的就是动量 Momentum, 这个变量是前期梯度的积累，当前梯度和前期梯度方向相同时，梯度更新会加速。当前梯度和前期梯度方向不同时，梯度更新会减缓。这样在转弯的时候，震荡就会减轻。

### 3.Adagrad

### 4.


### Reference
[1]. AdaDelta:AN ADAPTIVE Learning Rate Method, Matthew D. Zeiler, 2012
