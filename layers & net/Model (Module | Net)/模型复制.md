```py
import torch
import torch.nn as nn
from torch.optim import Adam

class Net(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(3,5,3)
    self.conv2 = nn.Conv2d(5,1,3)
  def forward(self,x):
    y = self.conv1(x)
    y = self.conv2(y)
    y = torch.sigmoid(y)
    return y

x = torch.randn(10,3,32,32)
net = Net()
y = net(x)
#print(y.shape)

y_t = torch.ones([10,1,28,28])
optimizer = Adam(net.parameters(),lr=0.02)
loss_f = nn.MSELoss()
#print(net.bias)


#参数实际上是：输出通道数*输入通道数*卷积核
#--------原始参数的一个输出通道：输入通道数*卷积核
for i in net.parameters():
	print(i[0]) 
	break

optimizer.zero_grad()
loss = loss_f(y,y_t)
loss.backward()
optimizer.step()
#print(loss)

#--------梯度更新后的一个输出通道：输入通道数*卷积核
for i in net.parameters():
	print(i[0]) 
	break

import copy
net_copy = copy.deepcopy(net)
print(net_copy)
for i in net_copy.parameters():
	print(i[0]) 
	break

net_dict = dict(net_copy.named_parameters())
print('##############')
print(net_dict)
```
